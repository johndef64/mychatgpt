{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-47K3jZbR10",
    "outputId": "2aac3585-5711-46fb-cb4d-d6f0b3d537a5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Download single GitHub file from repository\n",
    "def get_gitfile(url, flag='', dir = os.getcwd()):\n",
    "    url = url.replace('blob','raw')\n",
    "    response = requests.get(url)\n",
    "    file_name = flag + url.rsplit('/',1)[1]\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully. Saved as {file_name}\")\n",
    "    else:\n",
    "        print(\"Unable to download the file.\")\n",
    "\n",
    "# Get pychatgpt\n",
    "handle=\"https://raw.githubusercontent.com/johndef64/pychatgpt/main/\"\n",
    "files = [\"pychatgpt_static.py\",\"pychatgpt.py\" ]\n",
    "for file in files:\n",
    "    url = handle+file\n",
    "    get_gitfile(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Using gpt-4 model*\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import pychatgpt_stream as op\n",
    "\n",
    "op.choose_model()\n",
    "\n",
    "# wrap outputs for readability\n",
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chat examples\n",
    "op.get_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change API key function\n",
    "op.change_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file as variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "file = op.load_file(path)\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Load conversation? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*new chat*\n"
     ]
    }
   ],
   "source": [
    "if op.simple_bool('Load chat?'):\n",
    "    op.load_chat()\n",
    "    df = op.pd.DataFrame(op.chat_gpt)\n",
    "    print('\\n')\n",
    "    for i in range(len(df)):\n",
    "        print(df.role[i],':\\n', df.content[i])\n",
    "        print('-------------------------------------------------------------')\n",
    "else: \n",
    "    op.chat_gpt = []\n",
    "    print('*new chat*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand chat\n",
    "op.clearchat()\n",
    "a= ''\n",
    "b= ''\n",
    "c= ''\n",
    "if a != '': op.expand_chat(a, 'system')\n",
    "if b != '': op.expand_chat(b, 'assistant') \n",
    "if c != '': op.expand_chat(c, 'user') \n",
    "op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# continue chat\n",
    "system = '''\n",
    "\n",
    "'''\n",
    "#model = 'gpt-4'\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message,\n",
    "                    system=system,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. 広がりゆく 天の川ジャック 宇宙最強 超次元少女 \n",
      "   ひろがりゆく あまのがわじゃっく うちゅうさいきょう ちょうじげんしょうじょ\n",
      "   hirogariyuku amanogawajakku uchuusaikyou choujikenshoujo.\n",
      "   Traduzione: La Via Lattea si sta espandendo, Jack. Sono la ragazza più forte e ultradimensionale dell'universo.\n",
      "\n",
      "5. ウソ！それって本当の事なの？ げっちゅーもっと行くぞ 進行中 Let's go 最高！ \n",
      "   うそ！それってほんとうのことなの？ げっちゅーもっといくぞ しんこうちゅう れっつごーさいこう！\n",
      "   uso! sorette hontou no koto na no? getchuu motto ikuzo shinkou-chuu rettsugou saikou!\n",
      "   Traduzione: Davvero! È davvero la verità? Prenderò di più e continuo ad avanzare. Andiamo, è il massimo!\n",
      "\n",
      "6. まだ見ぬ Earth 探してユニヴァース 近くに来てるかも インベーダー 時を超えて伝えたい 繋ぎたい代々 \n",
      "   まだみぬ あーす たんさけてゆにばーす ちかくにきてるかも いんべーだー ときをこえてつたえたい つなぎたいだいだい\n",
      "   mada minu aasu tansakete yunibaasu chikaku ni kiteru ka mo inbeedaa toki o koete tsutaetai tsunagitai daidai.\n",
      "   Traduzione: Cercando una Terra ancora invisibile nell'Universo, potrei essere un invasore che si sta avvicinando. Voglio trasmetterlo attraverso il tempo, voglio connettere le generazioni.\n",
      "\n",
      "7. ここだよ まだ見えない？ 侵 略 完 了\n",
      "   ここだよ まだみえない？ しんりゃく かんりょう\n",
      "   koko da yo mada mienai? shinryaku kanryou.\n",
      "   Traduzione: Sono qui, non riesci ancora a vedermi? Invasione completata.\n",
      "\n",
      "Il testo sembra narrare la storia di una creatura extraterrestre che sta invadendo la Terra. Il tono è giocoso e combina elementi scientifici, esoterici e casuali, il che è tipico di molte canzoni pop in Giappone. Il linguaggio è informale e amichevole, con un uso creativo delle parole per creare immagini vivide e uniche. Questa è una caratteristica della canzone che la rende tanto più affascinante e memorabile nel contesto della musica pop giapponese.\n",
      "prompt tokens: 2105\n"
     ]
    }
   ],
   "source": [
    "#op.load_chat()\n",
    "m = '''\n",
    "completa il lavoro\n",
    "'''\n",
    "op.send_message(m, model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Conversation name: sys_Japanese_translator\n"
     ]
    }
   ],
   "source": [
    "op.save_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*chat cleared*\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who would you like to talk to? Joker (Batman)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are connecting you with Joker (Batman)...\n"
     ]
    }
   ],
   "source": [
    "op.clearchat()\n",
    "\n",
    "character = input('Who would you like to talk to?')\n",
    "print('we are connecting you with '+character+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, my dear friend, sanity is such a mundane concept, isn't it? But who needs sanity when you can have a bit of madness? Life is just a twisted game, and I am the jester who adds a touch of chaos to keep things interesting. So, to answer your question, sanity has no place in my world. But, tell me, why are you so interested in sanity?\n",
      "prompt tokens: 159\n"
     ]
    }
   ],
   "source": [
    "m = '''\n",
    "Hello, are you sane?\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()\n",
    "print(op.chat_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiple chats\n",
    "## chat 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_chat()\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.clearchat()\n",
    "sys_1 = '''\n",
    "\n",
    "'''\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message, system=sys_1, model= op.model)\n",
    "con1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_1 = op.chat_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_chat()\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "char_1 = \"\"\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "model = 'gpt-4' # change model\n",
    "maxtoken = 2000 # change maxtoken in response\n",
    "\n",
    "op.send_message(m,\n",
    "                persona= char_1, \n",
    "                model= op.model, \n",
    "                maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_2 = op.chat_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys_3 = '''  '''\n",
    "m = '''  '''\n",
    "op.choose_model()\n",
    "\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZEsrbmpUVkp",
    "outputId": "8d3d757c-042e-4553-ab28-9900ef8d194b"
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.chat_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTP vs GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "chat_1, chat_2 = [], []\n",
    "\n",
    "char_1 = 'Ghandi'\n",
    "char_2 = 'Hitler'\n",
    "\n",
    "maxtoken = 200\n",
    "iterations = 5\n",
    "sleep = 3\n",
    "\n",
    "# Seed message (char_2 to char_1)\n",
    "char_1_inci = 'Good morining '+ char_2\n",
    "char_2_reply = 'My pleasure to meet you '+ char_1\n",
    "char_1_reply = 'The pleasure is mine.'\n",
    "\n",
    "op.chat_gpt = chat_1   # assistant = char1\n",
    "op.expand_chat_assistant(char_1_inci) \n",
    "op.expand_chat_gpt(char_2_reply) \n",
    "chat_1 = op.chat_gpt\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_inci)\n",
    "\n",
    "op.chat_gpt = chat_2  # assistant = char2\n",
    "op.expand_chat_gpt(char_1_inci) \n",
    "op.expand_chat_assistant(char_2_reply) \n",
    "chat_2 = op.chat_gpt\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "print(char_2_reply)\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_reply)\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "op.chat_gpt = chat_2\n",
    "op.send_message(char_1_reply, model= op.model, persona= char_2,\n",
    "                maxtoken=maxtoken,printtoken=False)\n",
    "chat_2 = op.chat_gpt\n",
    "time.sleep(sleep)\n",
    "    \n",
    "i = 0\n",
    "while i in range(iterations):\n",
    "    \n",
    "    print('\\n'+ char_1+':')\n",
    "    op.chat_gpt = chat_1\n",
    "    op.send_message(op.reply, model= op.model, persona= char_1,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_1 = op.chat_gpt\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    print('\\n'+ char_2+':')\n",
    "    op.chat_gpt = chat_2\n",
    "    op.send_message(op.reply, model= op.model, persona= char_2,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_2 = op.chat_gpt\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op.save_chat()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
