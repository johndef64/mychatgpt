{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-47K3jZbR10",
    "outputId": "2aac3585-5711-46fb-cb4d-d6f0b3d537a5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Download single GitHub file from repository\n",
    "def get_gitfile(url, flag='', dir = os.getcwd()):\n",
    "    url = url.replace('blob','raw')\n",
    "    response = requests.get(url)\n",
    "    file_name = flag + url.rsplit('/',1)[1]\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully. Saved as {file_name}\")\n",
    "    else:\n",
    "        print(\"Unable to download the file.\")\n",
    "\n",
    "# Get pychatgpt\n",
    "handle=\"https://raw.githubusercontent.com/johndef64/pychatgpt/main/\"\n",
    "files = [\"pychatgpt.py\",\"pychatgpt_stream.py\" ]\n",
    "for file in files:\n",
    "    url = handle+file\n",
    "    get_gitfile(url)\n",
    "\n",
    "\n",
    "# Get conversation examples\n",
    "handle = \"https://github.com/johndef64/pychatgpt/blob/main/conversations/\"\n",
    "files=[\"sys_Python-Coding-Assistant\",\n",
    "       \"sys_Code-ngineer_(fast)\",\n",
    "       \"char_Nietzsche\",\n",
    "       \"chat_Confucius-vs-Chuangzi\"]\n",
    "       \n",
    "path = os.getcwd() + '/conversations'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "for file in files:\n",
    "    url = handle + file + '.json'\n",
    "    get_gitfile(url, dir=os.getcwd()+'/conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Using gpt-4 model*\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import pychatgpt as op\n",
    "\n",
    "op.choose_model()\n",
    "\n",
    "# wrap outputs for readability\n",
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change API key function\n",
    "op.change_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file as variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "file = op.load_file(path)\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Load conversation? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*new chat*\n"
     ]
    }
   ],
   "source": [
    "if op.simple_bool('Load conversation?'):\n",
    "    op.load_conversation()\n",
    "    df = op.pd.DataFrame(op.conversation_gpt)\n",
    "    print('\\n')\n",
    "    for i in range(len(df)):\n",
    "        print(df.role[i],':\\n', df.content[i])\n",
    "        print('-------------------------------------------------------------')\n",
    "else: \n",
    "    op.conversation_gpt = []\n",
    "    print('*new chat*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand conversation\n",
    "op.clearchat()\n",
    "a= ''\n",
    "b= ''\n",
    "c= ''\n",
    "if a != '': op.expand_conversation(a, 'system')\n",
    "if b != '': op.expand_conversation(b, 'assistant') \n",
    "if c != '': op.expand_conversation(c, 'user') \n",
    "op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 地球上のみんな\n",
      "   ちきゅうじょうのみんな\n",
      "   chikyuu jou no minna\n",
      "   Traduzione: Tutti sulla Terra\n",
      "   Preposizione: の (no) - indica una relazione di possesso o appartenenza.\n",
      "   Soggetto: みんな (minna) - tradotto come 'tutti'.\n",
      "\n",
      "2. 聞こえてる？\n",
      "   きこえてる？\n",
      "   kikoeteru?\n",
      "   Traduzione: Puoi sentire?\n",
      "   Verbo: 聞こえてる (kikoeteru) - forma in te-iru del verbo 'kikoeru' che significa 'sentire, ascoltare'.\n",
      "\n",
      "3. 早くこっちにおいでよ\n",
      "   はやくこっちにおいでよ\n",
      "   hayaku kocchi ni oide yo\n",
      "   Traduzione: Vieni qui in fretta\n",
      "   Avverbio: 早く (hayaku) - indica fretta.\n",
      "   Verbo: おいで (oide) - forma imperativa del verbo 'kuru' che significa 'venire'.\n",
      "   Particella: よ (yo) - usato alla fine della frase per enfatizzare un'istruzione o consiglio.\n",
      "\n",
      "4. 宇宙から交信\n",
      "   うちゅうからこうしん\n",
      "   uchuu kara koushin\n",
      "   Traduzione: Comunicazione dallo spazio\n",
      "   Preposizione: から (kara) - indica la provenienza.\n",
      " \n",
      "5. 今伝える摩訶不思議な宇宙のストーリー\n",
      "   いまつたえるまかふしぎなうちゅうのすとーりー\n",
      "   ima tsutaeru maka fushigi na uchuu no sutoorii\n",
      "   Traduzione: Ora racconto una storia misteriosa dello spazio\n",
      "   Avverbio: 今 (ima) - ora.\n",
      "   Verbo: 伝える (tsutaeru) - raccontare, rivelare.\n",
      "   Aggettivo: 摩訶不思議な (makafushigina) - misterioso.\n",
      "\n",
      "6. ムーンライト 信じて\n",
      "   むーんらいとしんじて\n",
      "   muunraito shinjite\n",
      "   Traduzione: Credi nella luce lunare\n",
      "   Verbo: 信じて (shinjite) - forma imperativa del verbo 'shinjiru' che significa 'credere'.\n",
      "   \n",
      "7. スターライズ いたのよ\n",
      "   すたーらいずいたのよ\n",
      "   sutaaraizu ita no yo\n",
      "   Traduzione: Ci sono le stelle\n",
      "   Verbo: いた (ita) - passato del verbo 'iru' che significa 'essere, esistere'.\n",
      "   \n",
      "8. サンシャイン 想いを\n",
      "   さんしゃいんおもいを\n",
      "   sanshain omoi o\n",
      "   Traduzione: I sentimenti al sole\n",
      "   Nome: 想い (omoi) - sentimenti.\n",
      "   \n",
      "9. シャイニング 届けて\n",
      "   しゃいにんぐとどけて\n",
      "   shainingu todokete\n",
      "   Traduzione: Fai brillare\n",
      "   Verbo: 届けて (todokete) - forma imperativa\n",
      "prompt tokens: 1214\n"
     ]
    }
   ],
   "source": [
    "# continue conversation\n",
    "system = '''\n",
    "\n",
    "'''\n",
    "#model = 'gpt-4'\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message,\n",
    "                    system=system,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. 広がりゆく 天の川ジャック 宇宙最強 超次元少女 \n",
      "   ひろがりゆく あまのがわじゃっく うちゅうさいきょう ちょうじげんしょうじょ\n",
      "   hirogariyuku amanogawajakku uchuusaikyou choujikenshoujo.\n",
      "   Traduzione: La Via Lattea si sta espandendo, Jack. Sono la ragazza più forte e ultradimensionale dell'universo.\n",
      "\n",
      "5. ウソ！それって本当の事なの？ げっちゅーもっと行くぞ 進行中 Let's go 最高！ \n",
      "   うそ！それってほんとうのことなの？ げっちゅーもっといくぞ しんこうちゅう れっつごーさいこう！\n",
      "   uso! sorette hontou no koto na no? getchuu motto ikuzo shinkou-chuu rettsugou saikou!\n",
      "   Traduzione: Davvero! È davvero la verità? Prenderò di più e continuo ad avanzare. Andiamo, è il massimo!\n",
      "\n",
      "6. まだ見ぬ Earth 探してユニヴァース 近くに来てるかも インベーダー 時を超えて伝えたい 繋ぎたい代々 \n",
      "   まだみぬ あーす たんさけてゆにばーす ちかくにきてるかも いんべーだー ときをこえてつたえたい つなぎたいだいだい\n",
      "   mada minu aasu tansakete yunibaasu chikaku ni kiteru ka mo inbeedaa toki o koete tsutaetai tsunagitai daidai.\n",
      "   Traduzione: Cercando una Terra ancora invisibile nell'Universo, potrei essere un invasore che si sta avvicinando. Voglio trasmetterlo attraverso il tempo, voglio connettere le generazioni.\n",
      "\n",
      "7. ここだよ まだ見えない？ 侵 略 完 了\n",
      "   ここだよ まだみえない？ しんりゃく かんりょう\n",
      "   koko da yo mada mienai? shinryaku kanryou.\n",
      "   Traduzione: Sono qui, non riesci ancora a vedermi? Invasione completata.\n",
      "\n",
      "Il testo sembra narrare la storia di una creatura extraterrestre che sta invadendo la Terra. Il tono è giocoso e combina elementi scientifici, esoterici e casuali, il che è tipico di molte canzoni pop in Giappone. Il linguaggio è informale e amichevole, con un uso creativo delle parole per creare immagini vivide e uniche. Questa è una caratteristica della canzone che la rende tanto più affascinante e memorabile nel contesto della musica pop giapponese.\n",
      "prompt tokens: 2105\n"
     ]
    }
   ],
   "source": [
    "#op.load_conversation()\n",
    "m = '''\n",
    "completa il lavoro\n",
    "'''\n",
    "op.send_message(m, model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Conversation name: sys_Japanese_translator\n"
     ]
    }
   ],
   "source": [
    "op.save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*chat cleared*\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who would you like to talk to? Joker (Batman)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are connecting you with Joker (Batman)...\n"
     ]
    }
   ],
   "source": [
    "op.clearchat()\n",
    "\n",
    "character = input('Who would you like to talk to?')\n",
    "print('we are connecting you with '+character+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, my dear friend, sanity is such a mundane concept, isn't it? But who needs sanity when you can have a bit of madness? Life is just a twisted game, and I am the jester who adds a touch of chaos to keep things interesting. So, to answer your question, sanity has no place in my world. But, tell me, why are you so interested in sanity?\n",
      "prompt tokens: 159\n"
     ]
    }
   ],
   "source": [
    "m = '''\n",
    "Hello, are you sane?\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()\n",
    "print(op.conversation_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiple conversations\n",
    "## conversation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_conversation()\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.clearchat()\n",
    "sys_1 = '''\n",
    "\n",
    "'''\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message, system=sys_1, model= op.model)\n",
    "con1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_1 = op.conversation_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_conversation()\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "char_1 = \"\"\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "model = 'gpt-4' # change model\n",
    "maxtoken = 2000 # change maxtoken in response\n",
    "\n",
    "op.send_message(m,\n",
    "                persona= char_1, \n",
    "                model= op.model, \n",
    "                maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_2 = op.conversation_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys_3 = '''  '''\n",
    "m = '''  '''\n",
    "op.choose_model()\n",
    "\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZEsrbmpUVkp",
    "outputId": "8d3d757c-042e-4553-ab28-9900ef8d194b"
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.conversation_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTP vs GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "chat_1, chat_2 = [], []\n",
    "\n",
    "char_1 = 'Ghandi'\n",
    "char_2 = 'Hitler'\n",
    "\n",
    "maxtoken = 200\n",
    "iterations = 5\n",
    "sleep = 3\n",
    "\n",
    "# Seed message (char_2 to char_1)\n",
    "char_1_inci = 'Good morining '+ char_2\n",
    "char_2_reply = 'My pleasure to meet you '+ char_1\n",
    "char_1_reply = 'The pleasure is mine.'\n",
    "\n",
    "op.conversation_gpt = chat_1   # assistant = char1\n",
    "op.expand_conversation_assistant(char_1_inci) \n",
    "op.expand_conversation_gpt(char_2_reply) \n",
    "chat_1 = op.conversation_gpt\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_inci)\n",
    "\n",
    "op.conversation_gpt = chat_2  # assistant = char2\n",
    "op.expand_conversation_gpt(char_1_inci) \n",
    "op.expand_conversation_assistant(char_2_reply) \n",
    "chat_2 = op.conversation_gpt\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "print(char_2_reply)\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_reply)\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "op.conversation_gpt = chat_2\n",
    "op.send_message(char_1_reply, model= op.model, persona= char_2,\n",
    "                maxtoken=maxtoken,printtoken=False)\n",
    "chat_2 = op.conversation_gpt\n",
    "time.sleep(sleep)\n",
    "    \n",
    "i = 0\n",
    "while i in range(iterations):\n",
    "    \n",
    "    print('\\n'+ char_1+':')\n",
    "    op.conversation_gpt = chat_1\n",
    "    op.send_message(op.reply, model= op.model, persona= char_1,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_1 = op.conversation_gpt\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    print('\\n'+ char_2+':')\n",
    "    op.conversation_gpt = chat_2\n",
    "    op.send_message(op.reply, model= op.model, persona= char_2,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_2 = op.conversation_gpt\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op.save_conversation()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
