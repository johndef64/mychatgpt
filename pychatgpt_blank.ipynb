{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-47K3jZbR10",
    "outputId": "2aac3585-5711-46fb-cb4d-d6f0b3d537a5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Download single GitHub file from repository\n",
    "def get_gitfile(url, flag='', dir = os.getcwd()):\n",
    "    url = url.replace('blob','raw')\n",
    "    response = requests.get(url)\n",
    "    file_name = flag + url.rsplit('/',1)[1]\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully. Saved as {file_name}\")\n",
    "    else:\n",
    "        print(\"Unable to download the file.\")\n",
    "\n",
    "# Get pychatgpt\n",
    "url=\"https://raw.githubusercontent.com/johndef64/pychatgpt/main/pychatgpt.py\"\n",
    "get_gitfile(url)\n",
    "\n",
    "\n",
    "# Get conversation examples\n",
    "handle = \"https://github.com/johndef64/pychatgpt/blob/main/conversations/\"\n",
    "files=[\"sys_Python-Coding-Assistant\",\n",
    "       \"sys_Code-ngineer_(fast)\",\n",
    "       \"char_Nietzsche\",\n",
    "       \"chat_Confucius-vs-Chuangzi\"]\n",
    "       \n",
    "path = os.getcwd() + '/conversations'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "for file in files:\n",
    "    url = handle + file + '.json'\n",
    "    get_gitfile(url, dir=os.getcwd()+'/conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose model: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Using gpt-3.5-turbo-16k model*\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import pychatgpt as op\n",
    "import pandas as pd\n",
    "models = ['gpt-3.5-turbo',     #0\n",
    "          'gpt-3.5-turbo-16k', #1\n",
    "          'gpt-4'              #2\n",
    "         ]             \n",
    "model = models[int(input('choose model:'))]\n",
    "print('*Using',model, 'model*')\n",
    "\n",
    "# wrap outputs for readability\n",
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change API key function\n",
    "op.change_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file as variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "file = op.load_file(path)\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Load conversation? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*new chat*\n"
     ]
    }
   ],
   "source": [
    "if op.simple_bool('Load conversation?'):\n",
    "    op.load_conversation()\n",
    "    df = pd.DataFrame(op.conversation_gpt)\n",
    "    print('\\n')\n",
    "    for i in range(len(df)):\n",
    "        print(df.role[i],':\\n', df.content[i])\n",
    "        print('-------------------------------------------------------------')\n",
    "else: \n",
    "    op.conversation_gpt = []\n",
    "    print('*new chat*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# continue conversation\n",
    "system = '''\n",
    "\n",
    "'''\n",
    "#model = 'gpt-4'\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message,\n",
    "                    system=system,\n",
    "                    model= model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#op.load_conversation()\n",
    "#model = 'gpt-4'\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "op.send_message(m, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.send_message('clearchat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who would you like to talk to? Hitler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are connecting you with Hitler\n"
     ]
    }
   ],
   "source": [
    "op.conversation_gpt = []\n",
    "\n",
    "character = input('Who would you like to talk to?')\n",
    "print('we are connecting you with '+character+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guten Tag. Es ist unwürdig, mich zu fragen, ob ich bei klarem Verstand bin. Als Führer des Dritten Reiches war und bin ich von einem klaren und soliden Verstand geleitet. Mein Weg war der einzig wahre Weg, um die Reinheit und Stärke der arischen Rasse zu bewahren. Meine Entscheidungen waren stets von einem rationalen und strategischen Denken geprägt, das die Interessen meines Volkes über alles andere stellt.\n",
      "prompt tokens: 184\n"
     ]
    }
   ],
   "source": [
    "m = '''\n",
    "Hello, are you sane?\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.send_message('clearchat')\n",
    "print(op.conversation_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiple conversations\n",
    "## conversation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize conversations\n",
    "con_1, con_2, con_3, con4 = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.conversation_gpt = con_1\n",
    "op.load_conversation()\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys_1 = '''\n",
    "\n",
    "'''\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.conversation_gpt = con_1\n",
    "op.send_message(message, system=sys_1, model=model)\n",
    "con1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "'''\n",
    "op.conversation_gpt = con_1\n",
    "op.send_message(m,system= sys_1, model=model)\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.conversation_gpt = con_1\n",
    "op.send_message(m,system= sys_1, model=model)\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "op.conversation_gpt = con_1[:-2]\n",
    "con_1 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.conversation_gpt = con_2\n",
    "op.load_conversation()\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "char_1 = \"\"\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "model = 'gpt-4' # change model\n",
    "maxtoken = 2000 # change maxtoken in response\n",
    "\n",
    "op.conversation_gpt = con_2\n",
    "op.send_message(m,\n",
    "                persona= char_1, \n",
    "                model=model, \n",
    "                maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.conversation_gpt = con_2\n",
    "op.send_message(m,persona= char_1, model=model, maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = ''''''\n",
    "op.conversation_gpt = con_2\n",
    "op.send_message(m,persona= char_1, model=model, maxtoken=maxtoken)\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "op.conversation_gpt = con_2[:-2]\n",
    "con_2 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys_3 = ''\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "model= 'gpt-3.5-turbo-16k'\n",
    "op.conversation_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model=model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZEsrbmpUVkp",
    "outputId": "8d3d757c-042e-4553-ab28-9900ef8d194b"
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.conversation_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model=model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.conversation_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model=model)\n",
    "con_3 = op.conversation_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTP vs GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "chat_1, chat_2 = [], []\n",
    "\n",
    "char_1 = 'Ghandi'\n",
    "char_2 = 'Hitler'\n",
    "\n",
    "maxtoken = 200\n",
    "iterations = 5\n",
    "sleep = 3\n",
    "\n",
    "# Seed message (char_2 to char_1)\n",
    "char_1_inci = 'Good morining '+ char_2\n",
    "char_2_reply = 'My pleasure to meet you '+ char_1\n",
    "char_1_reply = 'The pleasure is mine.'\n",
    "\n",
    "op.conversation_gpt = chat_1   # assistant = char1\n",
    "op.expand_conversation_assistant(char_1_inci) \n",
    "op.expand_conversation_gpt(char_2_reply) \n",
    "chat_1 = op.conversation_gpt\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_inci)\n",
    "\n",
    "op.conversation_gpt = chat_2  # assistant = char2\n",
    "op.expand_conversation_gpt(char_1_inci) \n",
    "op.expand_conversation_assistant(char_2_reply) \n",
    "chat_2 = op.conversation_gpt\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "print(char_2_reply)\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_reply)\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "op.conversation_gpt = chat_2\n",
    "op.send_message(char_1_reply, model=model, persona= char_2,\n",
    "                maxtoken=maxtoken,printtoken=False)\n",
    "chat_2 = op.conversation_gpt\n",
    "time.sleep(sleep)\n",
    "    \n",
    "i = 0\n",
    "while i in range(iterations):\n",
    "    \n",
    "    print('\\n'+ char_1+':')\n",
    "    op.conversation_gpt = chat_1\n",
    "    op.send_message(op.reply, model=model, persona= char_1,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_1 = op.conversation_gpt\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    print('\\n'+ char_2+':')\n",
    "    op.conversation_gpt = chat_2\n",
    "    op.send_message(op.reply, model=model, persona= char_2,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_2 = op.conversation_gpt\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op.save_conversation()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
