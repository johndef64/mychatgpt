{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-47K3jZbR10",
    "outputId": "2aac3585-5711-46fb-cb4d-d6f0b3d537a5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Download single GitHub file from repository\n",
    "def get_gitfile(url, flag='', dir = os.getcwd()):\n",
    "    url = url.replace('blob','raw')\n",
    "    response = requests.get(url)\n",
    "    file_name = flag + url.rsplit('/',1)[1]\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully. Saved as {file_name}\")\n",
    "    else:\n",
    "        print(\"Unable to download the file.\")\n",
    "\n",
    "# Get pychatgpt\n",
    "handle=\"https://raw.githubusercontent.com/johndef64/pychatgpt/main/\"\n",
    "files = [\"pychatgpt_static.py\",\"pychatgpt.py\" ]\n",
    "for file in files:\n",
    "    url = handle+file\n",
    "    get_gitfile(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import module\n",
    "import pychatgpt as op\n",
    "\n",
    "op.choose_model()\n",
    "\n",
    "# wrap outputs for readability\n",
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chat examples\n",
    "op.get_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change API key function\n",
    "op.change_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file as variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "file = op.load_file(path)\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if op.simple_bool('Load chat?'):\n",
    "    op.load_chat()\n",
    "    df = op.pd.DataFrame(op.chat_gpt)\n",
    "    print('\\n')\n",
    "    for i in range(len(df)):\n",
    "        print(df.role[i],':\\n', df.content[i])\n",
    "        print('-------------------------------------------------------------')\n",
    "else: \n",
    "    op.chat_gpt = []\n",
    "    print('*new chat*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand chat\n",
    "op.clearchat()\n",
    "a= ''\n",
    "b= ''\n",
    "c= ''\n",
    "if a != '': op.expand_chat(a, 'system')\n",
    "if b != '': op.expand_chat(b, 'assistant') \n",
    "if c != '': op.expand_chat(c, 'user') \n",
    "op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# continue chat\n",
    "system = '''\n",
    "\n",
    "'''\n",
    "#model = 'gpt-4'\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message,\n",
    "                    system=system,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#op.load_chat()\n",
    "m = '''\n",
    "completa il lavoro\n",
    "'''\n",
    "op.send_message(m, model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.clearchat()\n",
    "\n",
    "character = input('Who would you like to talk to?')\n",
    "print('we are connecting you with '+character+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "Hello, are you sane?\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "op.send_message(m,\n",
    "                    persona=character,\n",
    "                    model= op.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.save_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# cleatchat\n",
    "op.clearchat()\n",
    "print(op.chat_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiple chats\n",
    "## chat 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_chat()\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "op.clearchat()\n",
    "sys_1 = '''\n",
    "\n",
    "'''\n",
    "message = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(message, system=sys_1, model= op.model)\n",
    "con1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''\n",
    "\n",
    "'''\n",
    "\n",
    "op.send_message(m,system= sys_1, model= op.model)\n",
    "con_1 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_1 = op.chat_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.load_chat()\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "char_1 = \"\"\n",
    "m = '''\n",
    "\n",
    "'''\n",
    "model = 'gpt-4' # change model\n",
    "maxtoken = 2000 # change maxtoken in response\n",
    "\n",
    "op.send_message(m,\n",
    "                persona= char_1, \n",
    "                model= op.model, \n",
    "                maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "\n",
    "op.send_message(m,persona= char_1, model= op.model, maxtoken=maxtoken)\n",
    "con_2 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove last interaction\n",
    "con_2 = op.chat_gpt[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys_3 = '''  '''\n",
    "m = '''  '''\n",
    "op.choose_model()\n",
    "\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZEsrbmpUVkp",
    "outputId": "8d3d757c-042e-4553-ab28-9900ef8d194b"
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.chat_gpt = con_3\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = '''  '''\n",
    "op.send_message(m,system= sys_3, model= op.model)\n",
    "con_3 = op.chat_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTP vs GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "chat_1, chat_2 = [], []\n",
    "\n",
    "char_1 = 'Ghandi'\n",
    "char_2 = 'Hitler'\n",
    "\n",
    "maxtoken = 200\n",
    "iterations = 5\n",
    "sleep = 3\n",
    "\n",
    "# Seed message (char_2 to char_1)\n",
    "char_1_inci = 'Good morining '+ char_2\n",
    "char_2_reply = 'My pleasure to meet you '+ char_1\n",
    "char_1_reply = 'The pleasure is mine.'\n",
    "\n",
    "op.chat_gpt = chat_1   # assistant = char1\n",
    "op.expand_chat_assistant(char_1_inci) \n",
    "op.expand_chat_gpt(char_2_reply) \n",
    "chat_1 = op.chat_gpt\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_inci)\n",
    "\n",
    "op.chat_gpt = chat_2  # assistant = char2\n",
    "op.expand_chat_gpt(char_1_inci) \n",
    "op.expand_chat_assistant(char_2_reply) \n",
    "chat_2 = op.chat_gpt\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "print(char_2_reply)\n",
    "print('\\n'+ char_1+':')\n",
    "print(char_1_reply)\n",
    "\n",
    "print('\\n'+ char_2+':')\n",
    "op.chat_gpt = chat_2\n",
    "op.send_message(char_1_reply, model= op.model, persona= char_2,\n",
    "                maxtoken=maxtoken,printtoken=False)\n",
    "chat_2 = op.chat_gpt\n",
    "time.sleep(sleep)\n",
    "    \n",
    "i = 0\n",
    "while i in range(iterations):\n",
    "    \n",
    "    print('\\n'+ char_1+':')\n",
    "    op.chat_gpt = chat_1\n",
    "    op.send_message(op.reply, model= op.model, persona= char_1,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_1 = op.chat_gpt\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "    print('\\n'+ char_2+':')\n",
    "    op.chat_gpt = chat_2\n",
    "    op.send_message(op.reply, model= op.model, persona= char_2,\n",
    "                    maxtoken=maxtoken,printtoken=False)\n",
    "    chat_2 = op.chat_gpt\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op.save_chat()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
